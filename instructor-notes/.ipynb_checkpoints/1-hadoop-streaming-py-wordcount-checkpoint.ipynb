{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Short Course\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hadoop Distributed File System\n",
    "\n",
    "Hadoop Distributed File System (HDFS)\n",
    "\n",
    "HDFS is the primary distributed storage used by Hadoop applications. A HDFS cluster primarily consists of a NameNode that manages the file system metadata and DataNodes that store the actual data. The [HDFS Architecture Guide](http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html) describes HDFS in detail. To learn more about the interaction of users and administrators with HDFS, please refer to [HDFS User Guide](http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html). \n",
    "\n",
    "All HDFS commands are invoked by the **bin/hdfs** script. Running the hdfs script without any arguments prints the description for all commands. For all the commands, please refer to [HDFS Commands Reference](http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop_root = '/home/ubuntu/shortcourse/hadoop-2.7.1/'\n",
    "hadoop_start_hdfs_cmd = hadoop_root + 'sbin/start-dfs.sh'\n",
    "hadoop_stop_hdfs_cmd = hadoop_root + 'sbin/stop-dfs.sh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/03 08:28:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /home/ubuntu/shortcourse/hadoop-2.7.1/logs/hadoop-ubuntu-namenode-ubuntu.out\n",
      "localhost: starting datanode, logging to /home/ubuntu/shortcourse/hadoop-2.7.1/logs/hadoop-ubuntu-datanode-ubuntu.out\n",
      "localhost: Java HotSpot(TM) Server VM warning: You have loaded library /home/ubuntu/shortcourse/hadoop-2.7.1/lib/native/libhadoop.so.1.0.0 which might have disabled stack guard. The VM will try to fix the stack guard now.\n",
      "localhost: It's highly recommended that you fix the library with 'execstack -c <libfile>', or link it with '-z noexecstack'.\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /home/ubuntu/shortcourse/hadoop-2.7.1/logs/hadoop-ubuntu-secondarynamenode-ubuntu.out\n",
      "0.0.0.0: Java HotSpot(TM) Server VM warning: You have loaded library /home/ubuntu/shortcourse/hadoop-2.7.1/lib/native/libhadoop.so.1.0.0 which might have disabled stack guard. The VM will try to fix the stack guard now.\n",
      "0.0.0.0: It's highly recommended that you fix the library with 'execstack -c <libfile>', or link it with '-z noexecstack'.\n",
      "15/11/03 08:28:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# start the hadoop distributed file system\n",
    "! {hadoop_start_hdfs_cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160 NameNode\n",
      "5526 SecondaryNameNode\n",
      "5642 Jps\n",
      "5320 DataNode\n"
     ]
    }
   ],
   "source": [
    "# show the jave jvm process summary\n",
    "# You should see NamenNode, SecondaryNameNode, and DataNode\n",
    "! jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal file operations and data preparation for later example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/03 08:28:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "drwxr-xr-x   - ubuntu supergroup          0 2015-10-11 14:39 /user\n",
      "drwxr-xr-x   - ubuntu supergroup          0 2015-11-02 20:14 /user/ubuntu\n",
      "drwxr-xr-x   - ubuntu supergroup          0 2015-11-02 20:12 /user/ubuntu/input\n",
      "-rw-r--r--   1 ubuntu supergroup     167518 2015-11-02 20:12 /user/ubuntu/input/alice.txt\n",
      "-rw-r--r--   1 ubuntu supergroup     717574 2015-11-02 20:12 /user/ubuntu/input/pride-and-prejudice.txt\n",
      "-rw-r--r--   1 ubuntu supergroup     594933 2015-11-02 20:12 /user/ubuntu/input/sherlock-holmes.txt\n",
      "drwxr-xr-x   - ubuntu supergroup          0 2015-11-02 20:13 /user/ubuntu/output\n",
      "-rw-r--r--   1 ubuntu supergroup          0 2015-11-02 20:13 /user/ubuntu/output/_SUCCESS\n",
      "-rw-r--r--   1 ubuntu supergroup     279999 2015-11-02 20:13 /user/ubuntu/output/part-00000\n",
      "drwxr-xr-x   - ubuntu supergroup          0 2015-11-02 20:14 /user/ubuntu/output2\n"
     ]
    }
   ],
   "source": [
    "# list recursively everything under the root dir\n",
    "! {hadoop_root + 'bin/hdfs dfs -ls -R /'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download some files for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2015-11-03 08:28:52--  http://www.gutenberg.org/ebooks/1342.txt.utf-8\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: http://www.gutenberg.org/cache/epub/1342/pg1342.txt [following]\n",
      "--2015-11-03 08:28:53--  http://www.gutenberg.org/cache/epub/1342/pg1342.txt\n",
      "Reusing existing connection to www.gutenberg.org:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 717574 (701K) [text/plain]\n",
      "Saving to: ‘/home/ubuntu/shortcourse/data/wordcount/pride-and-prejudice.txt’\n",
      "\n",
      "100%[======================================>] 717,574     1003KB/s   in 0.7s   \n",
      "\n",
      "2015-11-03 08:28:54 (1003 KB/s) - ‘/home/ubuntu/shortcourse/data/wordcount/pride-and-prejudice.txt’ saved [717574/717574]\n",
      "\n",
      "--2015-11-03 08:28:54--  http://www.gutenberg.org/ebooks/11.txt.utf-8\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: http://www.gutenberg.org/cache/epub/11/pg11.txt [following]\n",
      "--2015-11-03 08:28:56--  http://www.gutenberg.org/cache/epub/11/pg11.txt\n",
      "Reusing existing connection to www.gutenberg.org:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 167518 (164K) [text/plain]\n",
      "Saving to: ‘/home/ubuntu/shortcourse/data/wordcount/alice.txt’\n",
      "\n",
      "100%[======================================>] 167,518     --.-K/s   in 0.1s    \n",
      "\n",
      "2015-11-03 08:28:57 (1.09 MB/s) - ‘/home/ubuntu/shortcourse/data/wordcount/alice.txt’ saved [167518/167518]\n",
      "\n",
      "--2015-11-03 08:28:57--  http://www.gutenberg.org/ebooks/1661.txt.utf-8\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: http://www.gutenberg.org/cache/epub/1661/pg1661.txt [following]\n",
      "--2015-11-03 08:28:58--  http://www.gutenberg.org/cache/epub/1661/pg1661.txt\n",
      "Reusing existing connection to www.gutenberg.org:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 594933 (581K) [text/plain]\n",
      "Saving to: ‘/home/ubuntu/shortcourse/data/wordcount/sherlock-holmes.txt’\n",
      "\n",
      "100%[======================================>] 594,933     2.23MB/s   in 0.3s   \n",
      "\n",
      "2015-11-03 08:28:58 (2.23 MB/s) - ‘/home/ubuntu/shortcourse/data/wordcount/sherlock-holmes.txt’ saved [594933/594933]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We will use three ebooks from Project Gutenberg for later example\n",
    "# Pride and Prejudice by Jane Austen: http://www.gutenberg.org/ebooks/1342.txt.utf-8\n",
    "! wget http://www.gutenberg.org/ebooks/1342.txt.utf-8 -O /home/ubuntu/shortcourse/data/wordcount/pride-and-prejudice.txt\n",
    "\n",
    "# Alice's Adventures in Wonderland by Lewis Carroll: http://www.gutenberg.org/ebooks/11.txt.utf-8\n",
    "! wget http://www.gutenberg.org/ebooks/11.txt.utf-8 -O /home/ubuntu/shortcourse/data/wordcount/alice.txt\n",
    "    \n",
    "# The Adventures of Sherlock Holmes by Arthur Conan Doyle: http://www.gutenberg.org/ebooks/1661.txt.utf-8\n",
    "! wget http://www.gutenberg.org/ebooks/1661.txt.utf-8 -O /home/ubuntu/shortcourse/data/wordcount/sherlock-holmes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/03 08:29:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/11/03 08:29:03 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/ubuntu/input\n",
      "15/11/03 08:29:03 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/ubuntu/output\n",
      "15/11/03 08:29:03 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/ubuntu/output2\n"
     ]
    }
   ],
   "source": [
    "# delete existing folders\n",
    "! {hadoop_root + 'bin/hdfs dfs -rm -R /user/ubuntu/*'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/03 08:29:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "# create input folder\n",
    "! {hadoop_root + 'bin/hdfs dfs -mkdir /user/ubuntu/input'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/03 08:29:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "# copy the three books to the input folder in HDFS\n",
    "! {hadoop_root + 'bin/hdfs dfs -copyFromLocal /home/ubuntu/shortcourse/data/wordcount/* /user/ubuntu/input/'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/03 08:29:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "drwxr-xr-x   - ubuntu supergroup          0 2015-11-03 08:29 input\n",
      "-rw-r--r--   1 ubuntu supergroup     167518 2015-11-03 08:29 input/alice.txt\n",
      "-rw-r--r--   1 ubuntu supergroup     717574 2015-11-03 08:29 input/pride-and-prejudice.txt\n",
      "-rw-r--r--   1 ubuntu supergroup     594933 2015-11-03 08:29 input/sherlock-holmes.txt\n"
     ]
    }
   ],
   "source": [
    "# show if the files are there\n",
    "! {hadoop_root + 'bin/hdfs dfs -ls -R'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. WordCount Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the single word frequency in the uploaded three books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Yarn, the resource allocator for Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /home/ubuntu/shortcourse/hadoop-2.7.1/logs/yarn-ubuntu-resourcemanager-ubuntu.out\n",
      "localhost: starting nodemanager, logging to /home/ubuntu/shortcourse/hadoop-2.7.1/logs/yarn-ubuntu-nodemanager-ubuntu.out\n"
     ]
    }
   ],
   "source": [
    "# start the hadoop distributed file system\n",
    "! {hadoop_root + 'sbin/start-yarn.sh'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\t1\r\n",
      "gators\t1\r\n",
      "gators\t1\r\n",
      "beat\t1\r\n",
      "everyone\t1\r\n",
      "go\t1\r\n",
      "glory\t1\r\n",
      "gators\t1\r\n"
     ]
    }
   ],
   "source": [
    "# wordcount 1 the scripts\n",
    "# Map: /home/ubuntu/shortcourse/notes/scripts/wordcount1/mapper.py\n",
    "# Test locally the map script\n",
    "! echo \"go gators gators beat everyone go glory gators\" | \\\n",
    "  /home/ubuntu/shortcourse/notes/scripts/wordcount1/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beat\t1\n",
      "everyone\t1\n",
      "gators\t3\n",
      "glory\t1\n",
      "go\t2\n"
     ]
    }
   ],
   "source": [
    "# Reduce: /home/ubuntu/shortcourse/notes/scripts/wordcount1/reducer.py\n",
    "# Test locally the reduce script\n",
    "! echo \"go gators gators beat everyone go glory gators\" | \\\n",
    "  /home/ubuntu/shortcourse/notes/scripts/wordcount1/mapper.py | \\\n",
    "  sort -k1,1 | \\\n",
    "  /home/ubuntu/shortcourse/notes/scripts/wordcount1/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/03 08:29:47 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "15/11/03 08:29:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/home/ubuntu/shortcourse/notes/scripts/wordcount1/mapper.py, /home/ubuntu/shortcourse/notes/scripts/wordcount1/reducer.py] [] /tmp/streamjob5390010843768271216.jar tmpDir=null\n",
      "15/11/03 08:29:48 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/11/03 08:29:48 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/11/03 08:29:48 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/11/03 08:29:49 INFO mapred.FileInputFormat: Total input paths to process : 3\n",
      "15/11/03 08:29:49 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "15/11/03 08:29:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local774667417_0001\n",
      "15/11/03 08:29:50 INFO mapred.LocalDistributedCacheManager: Localized file:/home/ubuntu/shortcourse/notes/scripts/wordcount1/mapper.py as file:/home/ubuntu/shortcourse/hadoop-2.7.1/hdfs-store/tmp/mapred/local/1446568189784/mapper.py\n",
      "15/11/03 08:29:50 INFO mapred.LocalDistributedCacheManager: Localized file:/home/ubuntu/shortcourse/notes/scripts/wordcount1/reducer.py as file:/home/ubuntu/shortcourse/hadoop-2.7.1/hdfs-store/tmp/mapred/local/1446568189785/reducer.py\n",
      "15/11/03 08:29:50 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/11/03 08:29:50 INFO mapreduce.Job: Running job: job_local774667417_0001\n",
      "15/11/03 08:29:50 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/11/03 08:29:50 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/11/03 08:29:50 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/11/03 08:29:50 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/11/03 08:29:50 INFO mapred.LocalJobRunner: Starting task: attempt_local774667417_0001_m_000000_0\n",
      "15/11/03 08:29:50 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/11/03 08:29:50 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/11/03 08:29:50 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/ubuntu/input/pride-and-prejudice.txt:0+717574\n",
      "15/11/03 08:29:50 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/11/03 08:29:50 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/11/03 08:29:50 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/11/03 08:29:50 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/11/03 08:29:50 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/11/03 08:29:50 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/11/03 08:29:50 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/11/03 08:29:50 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/shortcourse/notes/./mapper.py]\n",
      "15/11/03 08:29:50 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/11/03 08:29:50 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/11/03 08:29:50 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/11/03 08:29:50 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/11/03 08:29:50 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/11/03 08:29:50 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/11/03 08:29:50 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/11/03 08:29:50 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/11/03 08:29:50 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/11/03 08:29:50 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/11/03 08:29:50 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/11/03 08:29:50 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/11/03 08:29:51 INFO mapreduce.Job: Job job_local774667417_0001 running in uber mode : false\n",
      "15/11/03 08:29:51 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/11/03 08:29:51 INFO mapred.LineRecordReader: Found UTF-8 BOM and skipped it\n",
      "15/11/03 08:29:51 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:29:51 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:29:51 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:29:51 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:29:51 INFO streaming.PipeMapRed: Records R/W=2690/1\n",
      "15/11/03 08:29:51 INFO streaming.PipeMapRed: R/W/S=10000/78760/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:29:51 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/11/03 08:29:51 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/11/03 08:29:51 INFO mapred.LocalJobRunner: \n",
      "15/11/03 08:29:51 INFO mapred.MapTask: Starting flush of map output\n",
      "15/11/03 08:29:51 INFO mapred.MapTask: Spilling map output\n",
      "15/11/03 08:29:51 INFO mapred.MapTask: bufstart = 0; bufend = 950559; bufvoid = 104857600\n",
      "15/11/03 08:29:51 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25716048(102864192); length = 498349/6553600\n",
      "15/11/03 08:29:52 INFO mapred.MapTask: Finished spill 0\n",
      "15/11/03 08:29:52 INFO mapred.Task: Task:attempt_local774667417_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/11/03 08:29:52 INFO mapred.LocalJobRunner: Records R/W=2690/1\n",
      "15/11/03 08:29:52 INFO mapred.Task: Task 'attempt_local774667417_0001_m_000000_0' done.\n",
      "15/11/03 08:29:52 INFO mapred.LocalJobRunner: Finishing task: attempt_local774667417_0001_m_000000_0\n",
      "15/11/03 08:29:52 INFO mapred.LocalJobRunner: Starting task: attempt_local774667417_0001_m_000001_0\n",
      "15/11/03 08:29:52 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/11/03 08:29:52 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/11/03 08:29:52 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/ubuntu/input/sherlock-holmes.txt:0+594933\n",
      "15/11/03 08:29:52 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/11/03 08:29:52 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/11/03 08:29:52 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/11/03 08:29:52 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/11/03 08:29:52 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/11/03 08:29:52 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/11/03 08:29:52 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/11/03 08:29:52 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/shortcourse/notes/./mapper.py]\n",
      "15/11/03 08:29:52 INFO mapred.LineRecordReader: Found UTF-8 BOM and skipped it\n",
      "15/11/03 08:29:52 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:29:52 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:29:52 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:29:52 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:29:52 INFO streaming.PipeMapRed: Records R/W=3018/1\n",
      "15/11/03 08:29:52 INFO streaming.PipeMapRed: R/W/S=10000/59465/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:29:52 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/11/03 08:29:52 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/11/03 08:29:52 INFO mapred.LocalJobRunner: \n",
      "15/11/03 08:29:52 INFO mapred.MapTask: Starting flush of map output\n",
      "15/11/03 08:29:52 INFO mapred.MapTask: Spilling map output\n",
      "15/11/03 08:29:52 INFO mapred.MapTask: bufstart = 0; bufend = 793799; bufvoid = 104857600\n",
      "15/11/03 08:29:52 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25784268(103137072); length = 430129/6553600\n",
      "15/11/03 08:29:52 INFO mapred.MapTask: Finished spill 0\n",
      "15/11/03 08:29:52 INFO mapred.Task: Task:attempt_local774667417_0001_m_000001_0 is done. And is in the process of committing\n",
      "15/11/03 08:29:52 INFO mapred.LocalJobRunner: Records R/W=3018/1\n",
      "15/11/03 08:29:52 INFO mapred.Task: Task 'attempt_local774667417_0001_m_000001_0' done.\n",
      "15/11/03 08:29:52 INFO mapred.LocalJobRunner: Finishing task: attempt_local774667417_0001_m_000001_0\n",
      "15/11/03 08:29:52 INFO mapred.LocalJobRunner: Starting task: attempt_local774667417_0001_m_000002_0\n",
      "15/11/03 08:29:52 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/11/03 08:29:52 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/11/03 08:29:52 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/ubuntu/input/alice.txt:0+167518\n",
      "15/11/03 08:29:52 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/11/03 08:29:53 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/11/03 08:29:53 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/11/03 08:29:53 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/11/03 08:29:53 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/11/03 08:29:53 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/11/03 08:29:53 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/11/03 08:29:53 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/shortcourse/notes/./mapper.py]\n",
      "15/11/03 08:29:53 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/11/03 08:29:53 INFO mapred.LineRecordReader: Found UTF-8 BOM and skipped it\n",
      "15/11/03 08:29:53 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:29:53 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:29:53 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:29:53 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:29:53 INFO streaming.PipeMapRed: Records R/W=3022/1\n",
      "15/11/03 08:29:53 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/11/03 08:29:53 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/11/03 08:29:53 INFO mapred.LocalJobRunner: \n",
      "15/11/03 08:29:53 INFO mapred.MapTask: Starting flush of map output\n",
      "15/11/03 08:29:53 INFO mapred.MapTask: Spilling map output\n",
      "15/11/03 08:29:53 INFO mapred.MapTask: bufstart = 0; bufend = 220464; bufvoid = 104857600\n",
      "15/11/03 08:29:53 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26096556(104386224); length = 117841/6553600\n",
      "15/11/03 08:29:53 INFO mapred.MapTask: Finished spill 0\n",
      "15/11/03 08:29:53 INFO mapred.Task: Task:attempt_local774667417_0001_m_000002_0 is done. And is in the process of committing\n",
      "15/11/03 08:29:53 INFO mapred.LocalJobRunner: Records R/W=3022/1\n",
      "15/11/03 08:29:53 INFO mapred.Task: Task 'attempt_local774667417_0001_m_000002_0' done.\n",
      "15/11/03 08:29:53 INFO mapred.LocalJobRunner: Finishing task: attempt_local774667417_0001_m_000002_0\n",
      "15/11/03 08:29:53 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/11/03 08:29:53 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/11/03 08:29:53 INFO mapred.LocalJobRunner: Starting task: attempt_local774667417_0001_r_000000_0\n",
      "15/11/03 08:29:53 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/11/03 08:29:53 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/11/03 08:29:53 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@166bea0\n",
      "15/11/03 08:29:54 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334154944, maxSingleShuffleLimit=83538736, mergeThreshold=220542272, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/11/03 08:29:54 INFO reduce.EventFetcher: attempt_local774667417_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/11/03 08:29:54 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local774667417_0001_m_000002_0 decomp: 279388 len: 279392 to MEMORY\n",
      "15/11/03 08:29:54 INFO reduce.InMemoryMapOutput: Read 279388 bytes from map-output for attempt_local774667417_0001_m_000002_0\n",
      "15/11/03 08:29:54 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 279388, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->279388\n",
      "15/11/03 08:29:54 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local774667417_0001_m_000000_0 decomp: 1199737 len: 1199741 to MEMORY\n",
      "15/11/03 08:29:54 INFO reduce.InMemoryMapOutput: Read 1199737 bytes from map-output for attempt_local774667417_0001_m_000000_0\n",
      "15/11/03 08:29:54 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1199737, inMemoryMapOutputs.size() -> 2, commitMemory -> 279388, usedMemory ->1479125\n",
      "15/11/03 08:29:54 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local774667417_0001_m_000001_0 decomp: 1008867 len: 1008871 to MEMORY\n",
      "15/11/03 08:29:54 INFO reduce.InMemoryMapOutput: Read 1008867 bytes from map-output for attempt_local774667417_0001_m_000001_0\n",
      "15/11/03 08:29:54 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1008867, inMemoryMapOutputs.size() -> 3, commitMemory -> 1479125, usedMemory ->2487992\n",
      "15/11/03 08:29:54 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/11/03 08:29:54 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "15/11/03 08:29:54 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/11/03 08:29:54 INFO mapred.Merger: Merging 3 sorted segments\n",
      "15/11/03 08:29:54 INFO mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 2487968 bytes\n",
      "15/11/03 08:29:54 INFO reduce.MergeManagerImpl: Merged 3 segments, 2487992 bytes to disk to satisfy reduce memory limit\n",
      "15/11/03 08:29:54 INFO reduce.MergeManagerImpl: Merging 1 files, 2487992 bytes from disk\n",
      "15/11/03 08:29:54 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/11/03 08:29:54 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/11/03 08:29:54 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2487982 bytes\n",
      "15/11/03 08:29:54 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "15/11/03 08:29:54 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/shortcourse/notes/./reducer.py]\n",
      "15/11/03 08:29:54 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/11/03 08:29:54 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/11/03 08:29:54 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:29:54 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:29:54 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:29:54 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:29:54 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:29:55 INFO streaming.PipeMapRed: Records R/W=16105/1\n",
      "15/11/03 08:29:55 INFO streaming.PipeMapRed: R/W/S=100000/10015/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:29:55 INFO streaming.PipeMapRed: R/W/S=200000/20446/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:29:55 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/11/03 08:29:55 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/11/03 08:29:55 INFO mapred.Task: Task:attempt_local774667417_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/11/03 08:29:55 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "15/11/03 08:29:55 INFO mapred.Task: Task attempt_local774667417_0001_r_000000_0 is allowed to commit now\n",
      "15/11/03 08:29:55 INFO output.FileOutputCommitter: Saved output of task 'attempt_local774667417_0001_r_000000_0' to hdfs://localhost:54310/user/ubuntu/output/_temporary/0/task_local774667417_0001_r_000000\n",
      "15/11/03 08:29:55 INFO mapred.LocalJobRunner: Records R/W=16105/1 > reduce\n",
      "15/11/03 08:29:55 INFO mapred.Task: Task 'attempt_local774667417_0001_r_000000_0' done.\n",
      "15/11/03 08:29:55 INFO mapred.LocalJobRunner: Finishing task: attempt_local774667417_0001_r_000000_0\n",
      "15/11/03 08:29:55 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/11/03 08:29:56 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/11/03 08:29:56 INFO mapreduce.Job: Job job_local774667417_0001 completed successfully\n",
      "15/11/03 08:29:56 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4990001\n",
      "\t\tFILE: Number of bytes written=12117697\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4990131\n",
      "\t\tHDFS: Number of bytes written=279999\n",
      "\t\tHDFS: Number of read operations=33\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=30213\n",
      "\t\tMap output records=261582\n",
      "\t\tMap output bytes=1964822\n",
      "\t\tMap output materialized bytes=2488004\n",
      "\t\tInput split bytes=330\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=25985\n",
      "\t\tReduce shuffle bytes=2488004\n",
      "\t\tReduce input records=261582\n",
      "\t\tReduce output records=25985\n",
      "\t\tSpilled Records=523164\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=200\n",
      "\t\tTotal committed heap usage (bytes)=1325137920\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1480025\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=279999\n",
      "15/11/03 08:29:56 INFO streaming.StreamJob: Output directory: output\n"
     ]
    }
   ],
   "source": [
    "# run them with Hadoop against the uploaded three books\n",
    "cmd = hadoop_root + 'bin/hadoop jar ' + hadoop_root + 'hadoop-streaming-2.7.1.jar ' + \\\n",
    "    '-input input ' + \\\n",
    "    '-output output ' + \\\n",
    "    '-mapper /home/ubuntu/shortcourse/notes/scripts/wordcount1/mapper.py ' + \\\n",
    "    '-reducer /home/ubuntu/shortcourse/notes/scripts/wordcount1/reducer.py ' + \\\n",
    "    '-file /home/ubuntu/shortcourse/notes/scripts/wordcount1/mapper.py ' + \\\n",
    "    '-file /home/ubuntu/shortcourse/notes/scripts/wordcount1/reducer.py'\n",
    "\n",
    "! {cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/03 08:30:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "-rw-r--r--   1 ubuntu supergroup          0 2015-11-03 08:29 output/_SUCCESS\n",
      "-rw-r--r--   1 ubuntu supergroup     279999 2015-11-03 08:29 output/part-00000\n"
     ]
    }
   ],
   "source": [
    "# list the output\n",
    "! {hadoop_root + 'bin/hdfs dfs -ls -R output'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/03 08:30:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/11/03 08:30:44 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "yourself--and\t1\n",
      "yourself.\t4\n",
      "yourself.\"\t6\n",
      "yourself.'\t2\n",
      "yourself;\t1\n",
      "yourself?\t1\n",
      "yourself?\"\t1\n",
      "yourselves\t4\n",
      "yourselves,\t1\n",
      "yourselves?\t1\n",
      "youth\t9\n",
      "youth,\t9\n",
      "youth,'\t3\n",
      "youth?\"\t1\n",
      "youths\t1\n",
      "zero,\t1\n",
      "zero-point,\t1\n",
      "zest\t1\n",
      "zigzag\t1\n",
      "zigzag,\t1\n"
     ]
    }
   ],
   "source": [
    "# Let's see what's in the output file\n",
    "# delete if previous results exist\n",
    "! rm -rf /home/ubuntu/shortcourse/tmp/*\n",
    "! {hadoop_root + 'bin/hdfs dfs -copyToLocal output/part-00000 /home/ubuntu/shortcourse/tmp/wc1-part-00000'}\n",
    "! tail -n 20 /home/ubuntu/shortcourse/tmp/wc1-part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercise: WordCount2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the single word frequency, where the words are given in a pattern file. \n",
    "\n",
    "For example, given pattern.txt file, which contains: \n",
    "    \n",
    "    \"a b c d\"\n",
    "\n",
    "And the input file is: \n",
    "\n",
    "    \"d e a c f g h i a b c d\". \n",
    "\n",
    "Then the output shoule be:\n",
    "\n",
    "    \"a 1\n",
    "     b 1\n",
    "     c 2\n",
    "     d 2\"\n",
    "     \n",
    "Please copy the mapper.py and reduce.py from the first wordcount example to foler \"/home/ubuntu/shortcourse/notes/scripts/wordcount2/\". The pattern file is given in the wordcount2 folder with name \"wc2-pattern.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**:\n",
    "1. pass the pattern file using \"-file option\" and use -cmdenv to pass the file name as environment variable\n",
    "2. in the mapper, read the pattern file into a set\n",
    "3. only print out the words that exist in the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/03 08:31:24 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "15/11/03 08:31:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/home/ubuntu/shortcourse/notes/scripts/wordcount2/mapper.py, /home/ubuntu/shortcourse/notes/scripts/wordcount2/reducer.py, /home/ubuntu/shortcourse/notes/scripts/wordcount2/wc2-pattern.txt] [] /tmp/streamjob6344597239144833252.jar tmpDir=null\n",
      "15/11/03 08:31:25 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/11/03 08:31:25 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/11/03 08:31:25 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/11/03 08:31:26 INFO mapred.FileInputFormat: Total input paths to process : 3\n",
      "15/11/03 08:31:26 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "15/11/03 08:31:26 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1743492606_0001\n",
      "15/11/03 08:31:27 INFO mapred.LocalDistributedCacheManager: Localized file:/home/ubuntu/shortcourse/notes/scripts/wordcount2/mapper.py as file:/home/ubuntu/shortcourse/hadoop-2.7.1/hdfs-store/tmp/mapred/local/1446568287169/mapper.py\n",
      "15/11/03 08:31:27 INFO mapred.LocalDistributedCacheManager: Localized file:/home/ubuntu/shortcourse/notes/scripts/wordcount2/reducer.py as file:/home/ubuntu/shortcourse/hadoop-2.7.1/hdfs-store/tmp/mapred/local/1446568287170/reducer.py\n",
      "15/11/03 08:31:27 INFO mapred.LocalDistributedCacheManager: Localized file:/home/ubuntu/shortcourse/notes/scripts/wordcount2/wc2-pattern.txt as file:/home/ubuntu/shortcourse/hadoop-2.7.1/hdfs-store/tmp/mapred/local/1446568287171/wc2-pattern.txt\n",
      "15/11/03 08:31:27 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/11/03 08:31:27 INFO mapreduce.Job: Running job: job_local1743492606_0001\n",
      "15/11/03 08:31:27 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/11/03 08:31:27 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/11/03 08:31:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/11/03 08:31:27 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/11/03 08:31:27 INFO mapred.LocalJobRunner: Starting task: attempt_local1743492606_0001_m_000000_0\n",
      "15/11/03 08:31:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/11/03 08:31:27 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/11/03 08:31:27 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/ubuntu/input/pride-and-prejudice.txt:0+717574\n",
      "15/11/03 08:31:27 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/11/03 08:31:27 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/11/03 08:31:27 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/11/03 08:31:27 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/11/03 08:31:27 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/11/03 08:31:27 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/11/03 08:31:27 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/11/03 08:31:27 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/shortcourse/notes/./mapper.py]\n",
      "15/11/03 08:31:27 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/11/03 08:31:27 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/11/03 08:31:27 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/11/03 08:31:27 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/11/03 08:31:27 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/11/03 08:31:27 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/11/03 08:31:27 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/11/03 08:31:27 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/11/03 08:31:27 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/11/03 08:31:27 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/11/03 08:31:27 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/11/03 08:31:27 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/11/03 08:31:27 INFO mapred.LineRecordReader: Found UTF-8 BOM and skipped it\n",
      "15/11/03 08:31:27 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:31:27 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:31:27 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:31:27 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:31:27 INFO streaming.PipeMapRed: Records R/W=2690/1\n",
      "15/11/03 08:31:28 INFO streaming.PipeMapRed: R/W/S=10000/23842/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:31:28 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/11/03 08:31:28 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/11/03 08:31:28 INFO mapred.LocalJobRunner: \n",
      "15/11/03 08:31:28 INFO mapred.MapTask: Starting flush of map output\n",
      "15/11/03 08:31:28 INFO mapred.MapTask: Spilling map output\n",
      "15/11/03 08:31:28 INFO mapred.MapTask: bufstart = 0; bufend = 204177; bufvoid = 104857600\n",
      "15/11/03 08:31:28 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26065732(104262928); length = 148665/6553600\n",
      "15/11/03 08:31:28 INFO mapred.MapTask: Finished spill 0\n",
      "15/11/03 08:31:28 INFO mapred.Task: Task:attempt_local1743492606_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/11/03 08:31:28 INFO mapred.LocalJobRunner: Records R/W=2690/1\n",
      "15/11/03 08:31:28 INFO mapred.Task: Task 'attempt_local1743492606_0001_m_000000_0' done.\n",
      "15/11/03 08:31:28 INFO mapred.LocalJobRunner: Finishing task: attempt_local1743492606_0001_m_000000_0\n",
      "15/11/03 08:31:28 INFO mapred.LocalJobRunner: Starting task: attempt_local1743492606_0001_m_000001_0\n",
      "15/11/03 08:31:28 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/11/03 08:31:28 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/11/03 08:31:28 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/ubuntu/input/sherlock-holmes.txt:0+594933\n",
      "15/11/03 08:31:28 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/11/03 08:31:28 INFO mapreduce.Job: Job job_local1743492606_0001 running in uber mode : false\n",
      "15/11/03 08:31:28 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/11/03 08:31:28 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/11/03 08:31:28 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/11/03 08:31:28 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/11/03 08:31:28 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/11/03 08:31:28 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/11/03 08:31:28 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/11/03 08:31:28 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/shortcourse/notes/./mapper.py]\n",
      "15/11/03 08:31:28 INFO mapred.LineRecordReader: Found UTF-8 BOM and skipped it\n",
      "15/11/03 08:31:28 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:31:28 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:31:28 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:31:28 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:31:28 INFO streaming.PipeMapRed: Records R/W=3018/1\n",
      "15/11/03 08:31:28 INFO streaming.PipeMapRed: R/W/S=10000/17217/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:31:28 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/11/03 08:31:28 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/11/03 08:31:28 INFO mapred.LocalJobRunner: \n",
      "15/11/03 08:31:28 INFO mapred.MapTask: Starting flush of map output\n",
      "15/11/03 08:31:28 INFO mapred.MapTask: Spilling map output\n",
      "15/11/03 08:31:28 INFO mapred.MapTask: bufstart = 0; bufend = 172324; bufvoid = 104857600\n",
      "15/11/03 08:31:28 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26087308(104349232); length = 127089/6553600\n",
      "15/11/03 08:31:28 INFO mapred.MapTask: Finished spill 0\n",
      "15/11/03 08:31:29 INFO mapred.Task: Task:attempt_local1743492606_0001_m_000001_0 is done. And is in the process of committing\n",
      "15/11/03 08:31:29 INFO mapred.LocalJobRunner: Records R/W=3018/1\n",
      "15/11/03 08:31:29 INFO mapred.Task: Task 'attempt_local1743492606_0001_m_000001_0' done.\n",
      "15/11/03 08:31:29 INFO mapred.LocalJobRunner: Finishing task: attempt_local1743492606_0001_m_000001_0\n",
      "15/11/03 08:31:29 INFO mapred.LocalJobRunner: Starting task: attempt_local1743492606_0001_m_000002_0\n",
      "15/11/03 08:31:29 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/11/03 08:31:29 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/11/03 08:31:29 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/ubuntu/input/alice.txt:0+167518\n",
      "15/11/03 08:31:29 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/11/03 08:31:29 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/11/03 08:31:29 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/11/03 08:31:29 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/11/03 08:31:29 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/11/03 08:31:29 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/11/03 08:31:29 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/11/03 08:31:29 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/shortcourse/notes/./mapper.py]\n",
      "15/11/03 08:31:29 INFO mapred.LineRecordReader: Found UTF-8 BOM and skipped it\n",
      "15/11/03 08:31:29 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:31:29 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:31:29 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:31:29 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:31:29 INFO streaming.PipeMapRed: Records R/W=3022/1\n",
      "15/11/03 08:31:29 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/11/03 08:31:29 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/11/03 08:31:29 INFO mapred.LocalJobRunner: \n",
      "15/11/03 08:31:29 INFO mapred.MapTask: Starting flush of map output\n",
      "15/11/03 08:31:29 INFO mapred.MapTask: Spilling map output\n",
      "15/11/03 08:31:29 INFO mapred.MapTask: bufstart = 0; bufend = 44784; bufvoid = 104857600\n",
      "15/11/03 08:31:29 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26181860(104727440); length = 32537/6553600\n",
      "15/11/03 08:31:29 INFO mapred.MapTask: Finished spill 0\n",
      "15/11/03 08:31:29 INFO mapred.Task: Task:attempt_local1743492606_0001_m_000002_0 is done. And is in the process of committing\n",
      "15/11/03 08:31:29 INFO mapred.LocalJobRunner: Records R/W=3022/1\n",
      "15/11/03 08:31:29 INFO mapred.Task: Task 'attempt_local1743492606_0001_m_000002_0' done.\n",
      "15/11/03 08:31:29 INFO mapred.LocalJobRunner: Finishing task: attempt_local1743492606_0001_m_000002_0\n",
      "15/11/03 08:31:29 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/11/03 08:31:29 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/11/03 08:31:29 INFO mapred.LocalJobRunner: Starting task: attempt_local1743492606_0001_r_000000_0\n",
      "15/11/03 08:31:29 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/11/03 08:31:29 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/11/03 08:31:29 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1ea672d\n",
      "15/11/03 08:31:29 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334154944, maxSingleShuffleLimit=83538736, mergeThreshold=220542272, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/11/03 08:31:29 INFO reduce.EventFetcher: attempt_local1743492606_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/11/03 08:31:29 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1743492606_0001_m_000001_0 decomp: 235872 len: 235876 to MEMORY\n",
      "15/11/03 08:31:29 INFO reduce.InMemoryMapOutput: Read 235872 bytes from map-output for attempt_local1743492606_0001_m_000001_0\n",
      "15/11/03 08:31:29 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 235872, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->235872\n",
      "15/11/03 08:31:29 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1743492606_0001_m_000002_0 decomp: 61056 len: 61060 to MEMORY\n",
      "15/11/03 08:31:29 INFO reduce.InMemoryMapOutput: Read 61056 bytes from map-output for attempt_local1743492606_0001_m_000002_0\n",
      "15/11/03 08:31:29 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 61056, inMemoryMapOutputs.size() -> 2, commitMemory -> 235872, usedMemory ->296928\n",
      "15/11/03 08:31:29 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1743492606_0001_m_000000_0 decomp: 278513 len: 278517 to MEMORY\n",
      "15/11/03 08:31:29 INFO reduce.InMemoryMapOutput: Read 278513 bytes from map-output for attempt_local1743492606_0001_m_000000_0\n",
      "15/11/03 08:31:29 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 278513, inMemoryMapOutputs.size() -> 3, commitMemory -> 296928, usedMemory ->575441\n",
      "15/11/03 08:31:29 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/11/03 08:31:29 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "15/11/03 08:31:29 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/11/03 08:31:29 INFO mapred.Merger: Merging 3 sorted segments\n",
      "15/11/03 08:31:29 INFO mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 575429 bytes\n",
      "15/11/03 08:31:29 INFO reduce.MergeManagerImpl: Merged 3 segments, 575441 bytes to disk to satisfy reduce memory limit\n",
      "15/11/03 08:31:29 INFO reduce.MergeManagerImpl: Merging 1 files, 575441 bytes from disk\n",
      "15/11/03 08:31:29 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/11/03 08:31:29 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/11/03 08:31:29 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 575433 bytes\n",
      "15/11/03 08:31:29 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "15/11/03 08:31:29 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/shortcourse/notes/./reducer.py]\n",
      "15/11/03 08:31:29 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/11/03 08:31:29 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/11/03 08:31:29 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:31:29 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:31:29 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:31:29 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:31:29 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/11/03 08:31:30 INFO streaming.PipeMapRed: Records R/W=77075/1\n",
      "15/11/03 08:31:30 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/11/03 08:31:30 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/11/03 08:31:30 INFO mapred.Task: Task:attempt_local1743492606_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/11/03 08:31:30 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "15/11/03 08:31:30 INFO mapred.Task: Task attempt_local1743492606_0001_r_000000_0 is allowed to commit now\n",
      "15/11/03 08:31:30 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1743492606_0001_r_000000_0' to hdfs://localhost:54310/user/ubuntu/output2/_temporary/0/task_local1743492606_0001_r_000000\n",
      "15/11/03 08:31:30 INFO mapred.LocalJobRunner: Records R/W=77075/1 > reduce\n",
      "15/11/03 08:31:30 INFO mapred.Task: Task 'attempt_local1743492606_0001_r_000000_0' done.\n",
      "15/11/03 08:31:30 INFO mapred.LocalJobRunner: Finishing task: attempt_local1743492606_0001_r_000000_0\n",
      "15/11/03 08:31:30 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/11/03 08:31:30 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/11/03 08:31:30 INFO mapreduce.Job: Job job_local1743492606_0001 completed successfully\n",
      "15/11/03 08:31:30 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1167375\n",
      "\t\tFILE: Number of bytes written=3775289\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4990131\n",
      "\t\tHDFS: Number of bytes written=172\n",
      "\t\tHDFS: Number of read operations=33\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=30213\n",
      "\t\tMap output records=77075\n",
      "\t\tMap output bytes=421285\n",
      "\t\tMap output materialized bytes=575453\n",
      "\t\tInput split bytes=330\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce shuffle bytes=575453\n",
      "\t\tReduce input records=77075\n",
      "\t\tReduce output records=20\n",
      "\t\tSpilled Records=154150\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=178\n",
      "\t\tTotal committed heap usage (bytes)=1031274496\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1480025\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=172\n",
      "15/11/03 08:31:30 INFO streaming.StreamJob: Output directory: output2\n"
     ]
    }
   ],
   "source": [
    "# execise: count the words existing in the given pattern file for the three books\n",
    "\n",
    "cmd = hadoop_root + 'bin/hadoop jar ' + hadoop_root + 'hadoop-streaming-2.7.1.jar ' + \\\n",
    "    '-cmdenv PATTERN_FILE=wc2-pattern.txt ' + \\\n",
    "    '-input input ' + \\\n",
    "    '-output output2 ' + \\\n",
    "    '-mapper /home/ubuntu/shortcourse/notes/scripts/wordcount2/mapper.py ' + \\\n",
    "    '-reducer /home/ubuntu/shortcourse/notes/scripts/wordcount2/reducer.py ' + \\\n",
    "    '-file /home/ubuntu/shortcourse/notes/scripts/wordcount2/mapper.py ' + \\\n",
    "    '-file /home/ubuntu/shortcourse/notes/scripts/wordcount2/reducer.py ' + \\\n",
    "    '-file /home/ubuntu/shortcourse/notes/scripts/wordcount2/wc2-pattern.txt'\n",
    "\n",
    "! {cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Verify Results**\n",
    "\n",
    "1. Copy the output file to local\n",
    "2. run the following command, and compare with the downloaded output\n",
    "    \n",
    "    sort -nrk 2,2 part-00000  | head -n 20\n",
    "    \n",
    "The wc1-part-00000 is the output of the previous wordcount (wordcount1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/03 08:32:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/11/03 08:32:05 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "the\t11273\n",
      "to\t7594\n",
      "of\t6978\n",
      "and\t6887\n",
      "a\t5182\n",
      "I\t4533\n",
      "in\t3916\n",
      "was\t3484\n",
      "that\t3204\n",
      "her\t2428\n",
      "his\t2356\n",
      "you\t2317\n",
      "it\t2284\n",
      "he\t2148\n",
      "as\t2141\n",
      "had\t2107\n",
      "with\t2097\n",
      "she\t2095\n",
      "not\t2076\n",
      "be\t1975\n"
     ]
    }
   ],
   "source": [
    "! rm -rf /home/ubuntu/shortcourse/tmp/wc2-part-00000\n",
    "! {hadoop_root + 'bin/hdfs dfs -copyToLocal output2/part-00000 /home/ubuntu/shortcourse/tmp/wc2-part-00000'}\n",
    "! cat /home/ubuntu/shortcourse/tmp/wc2-part-00000 | sort -nrk2,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t11273\r\n",
      "to\t7594\r\n",
      "of\t6978\r\n",
      "and\t6887\r\n",
      "a\t5182\r\n",
      "I\t4533\r\n",
      "in\t3916\r\n",
      "was\t3484\r\n",
      "that\t3204\r\n",
      "her\t2428\r\n",
      "his\t2356\r\n",
      "you\t2317\r\n",
      "it\t2284\r\n",
      "he\t2148\r\n",
      "as\t2141\r\n",
      "had\t2107\r\n",
      "with\t2097\r\n",
      "she\t2095\r\n",
      "not\t2076\r\n",
      "be\t1975\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "! sort -nr -k2,2 /home/ubuntu/shortcourse/tmp/wc1-part-00000 | head -n 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n"
     ]
    }
   ],
   "source": [
    "# stop dfs and yarn\n",
    "!{hadoop_root + 'sbin/stop-yarn.sh'}\n",
    "# don't stop hdfs for now, later use\n",
    "# !{hadoop_stop_hdfs_cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
