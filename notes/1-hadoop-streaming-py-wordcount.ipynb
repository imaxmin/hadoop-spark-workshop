{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Short Course\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hadoop Distributed File System\n",
    "\n",
    "Hadoop Distributed File System (HDFS)\n",
    "\n",
    "HDFS is the primary distributed storage used by Hadoop applications. A HDFS cluster primarily consists of a NameNode that manages the file system metadata and DataNodes that store the actual data. The [HDFS Architecture Guide](http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html) describes HDFS in detail. To learn more about the interaction of users and administrators with HDFS, please refer to [HDFS User Guide](http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html). \n",
    "\n",
    "All HDFS commands are invoked by the **bin/hdfs** script. Running the hdfs script without any arguments prints the description for all commands. For all the commands, please refer to [HDFS Commands Reference](http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop_root = '/home/ubuntu/shortcourse/hadoop-2.7.1/'\n",
    "hadoop_start_hdfs_cmd = hadoop_root + 'sbin/start-dfs.sh'\n",
    "hadoop_stop_hdfs_cmd = hadoop_root + 'sbin/stop-dfs.sh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start the hadoop distributed file system\n",
    "! {hadoop_start_hdfs_cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show the jave jvm process summary\n",
    "# You should see NamenNode, SecondaryNameNode, and DataNode\n",
    "! jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal file operations and data preparation for later example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "list recursively everything under the root dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download some files for later use. The files should already be there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will use three ebooks from Project Gutenberg for later example\n",
    "# Pride and Prejudice by Jane Austen: http://www.gutenberg.org/ebooks/1342.txt.utf-8\n",
    "! wget http://www.gutenberg.org/ebooks/1342.txt.utf-8 -O /home/ubuntu/shortcourse/data/wordcount/pride-and-prejudice.txt\n",
    "\n",
    "# Alice's Adventures in Wonderland by Lewis Carroll: http://www.gutenberg.org/ebooks/11.txt.utf-8\n",
    "! wget http://www.gutenberg.org/ebooks/11.txt.utf-8 -O /home/ubuntu/shortcourse/data/wordcount/alice.txt\n",
    "    \n",
    "# The Adventures of Sherlock Holmes by Arthur Conan Doyle: http://www.gutenberg.org/ebooks/1661.txt.utf-8\n",
    "! wget http://www.gutenberg.org/ebooks/1661.txt.utf-8 -O /home/ubuntu/shortcourse/data/wordcount/sherlock-holmes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete existing folders under /user/ubuntu/ in hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create input folder: /user/ubuntu/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Copy the three books to the input folder in HDFS.\n",
    "Similiar to normal bash cmd:  \n",
    "    \n",
    "    cp /home/ubuntu/shortcourse/data/wordcount/* /user/ubuntu/input/\n",
    "    \n",
    "but copy to hdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Show if the files are there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. WordCount Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the single word frequency in the uploaded three books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Yarn, the resource allocator for Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Start the hadoop distributed file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! {hadoop_root + 'sbin/start-yarn.sh'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test locally the mapper.py and reduce.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wordcount 1 the scripts\n",
    "# Map: /home/ubuntu/shortcourse/notes/scripts/wordcount1/mapper.py\n",
    "# Test locally the map script\n",
    "! echo \"go gators gators beat everyone go glory gators\" | \\\n",
    "  /home/ubuntu/shortcourse/notes/scripts/wordcount1/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reduce: /home/ubuntu/shortcourse/notes/scripts/wordcount1/reducer.py\n",
    "# Test locally the reduce script\n",
    "! echo \"go gators gators beat everyone go glory gators\" | \\\n",
    "  /home/ubuntu/shortcourse/notes/scripts/wordcount1/mapper.py | \\\n",
    "  sort -k1,1 | \\\n",
    "  /home/ubuntu/shortcourse/notes/scripts/wordcount1/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run them with Hadoop against the uploaded three books\n",
    "cmd = hadoop_root + 'bin/hadoop jar ' + hadoop_root + 'hadoop-streaming-2.7.1.jar ' + \\\n",
    "    '-input input ' + \\\n",
    "    '-output output ' + \\\n",
    "    '-mapper /home/ubuntu/shortcourse/notes/scripts/wordcount1/mapper.py ' + \\\n",
    "    '-reducer /home/ubuntu/shortcourse/notes/scripts/wordcount1/reducer.py ' + \\\n",
    "    '-file /home/ubuntu/shortcourse/notes/scripts/wordcount1/mapper.py ' + \\\n",
    "    '-file /home/ubuntu/shortcourse/notes/scripts/wordcount1/reducer.py'\n",
    "\n",
    "! {cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the output file (part-00000) to local fs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see what's in the output file\n",
    "# delete if previous results exist\n",
    "! tail -n 20 $(THE_DOWNLOADED_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercise: WordCount2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the single word frequency, where the words are given in a pattern file. \n",
    "\n",
    "For example, given pattern.txt file, which contains: \n",
    "    \n",
    "    \"a b c d\"\n",
    "\n",
    "And the input file is: \n",
    "\n",
    "    \"d e a c f g h i a b c d\". \n",
    "\n",
    "Then the output shoule be:\n",
    "\n",
    "    \"a 1\n",
    "     b 1\n",
    "     c 2\n",
    "     d 2\"\n",
    "     \n",
    "Please copy the mapper.py and reduce.py from the first wordcount example to foler \"/home/ubuntu/shortcourse/notes/scripts/wordcount2/\". The pattern file is given in the wordcount2 folder with name \"wc2-pattern.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**:\n",
    "1. pass the pattern file using \"-file option\" and use -cmdenv to pass the file name as environment variable\n",
    "2. in the mapper, read the pattern file into a set\n",
    "3. only print out the words that exist in the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. go to wordcount2 folder, modify the mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. test locally if the mapper is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. run with hadoop streaming. Input is still the three books, output to 'output2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Verify Results**\n",
    "\n",
    "1. Copy the output file to local\n",
    "2. run the following command, and compare with the downloaded output\n",
    "    \n",
    "    sort -nrk 2,2 part-00000  | head -n 20\n",
    "    \n",
    "The wc1-part-00000 is the output of the previous wordcount (wordcount1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. list the output, download the output to local, and cat the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2. use bash cmd to find out the most frequently used 20 words from the previous example, \n",
    "#    and compare the results with this output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stop dfs and yarn\n",
    "!{hadoop_root + 'sbin/stop-yarn.sh'}\n",
    "# don't stop hdfs for now, later use\n",
    "# !{hadoop_stop_hdfs_cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
